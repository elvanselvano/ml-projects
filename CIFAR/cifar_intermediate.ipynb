{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar_intermediate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z3n_M9kzhdd"
      },
      "source": [
        "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
        "\n",
        "b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember\n",
        "\n",
        "to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
        "\n",
        "c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
        "\n",
        "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self- normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
        "\n",
        "e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
        "\n",
        "f. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28sh1RU03ff"
      },
      "source": [
        "## **a.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTqfKKWuy5mE"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100,\n",
        "                                 activation=\"elu\",\n",
        "                                 kernel_initializer=\"he_normal\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgfSJu4R06aD"
      },
      "source": [
        "## **b.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo7tQlO20-Jz"
      },
      "source": [
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "optimizer = keras.optimizers.Nadam(5e-5)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxLQjgyt1clS"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "X_train = X_train_full[5000:]\n",
        "y_train = y_train_full[5000:]\n",
        "X_valid = X_train_full[:5000]\n",
        "y_valid = y_train_full[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiWVrUJP1pXi"
      },
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
        "# setup tensorboard = ...\n",
        "# pake custom cb yang lihat ratio train acc dan val acc\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-sYGJKl2Mng"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=100,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          callbacks=callbacks)\n",
        "# epoch 47: loss: 1.1925 - accuracy: 0.5701 - val_loss: 1.5532 - val_accuracy: 0.4686"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA2vQK6p2UG3"
      },
      "source": [
        "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDQ_X1aO3iil"
      },
      "source": [
        "## **c.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8bMl5xH4AhM"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Activation(\"elu\"))\n",
        "\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "optimizer = keras.optimizers.Nadam(5e-4) # compare 5e-3, 5e-4, 5e-5\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
        "# setup tensorboard = ...\n",
        "# pake custom cb yang lihat ratio train acc dan val acc\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
        "\n",
        "# try to use dropout and compare performance\n",
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD7zhYAG6ISH"
      },
      "source": [
        "Is it converging faster than before?  Yes, BN stabilized training and allowed us to use bigger LR -> faster convergence.\n",
        "\n",
        "Does it produce a better model? Yes.\n",
        "\n",
        "How does it affect training speed? Slower, because of extra computation in BN layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw_vwIii72hs"
      },
      "source": [
        "# **d.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev1XTxKI744v"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# you can tune this architecture using hyperparameter library\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100,\n",
        "                                 kernel_initializer=\"lecun_normal\",\n",
        "                                 activation=\"selu\"))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(lr=7e-4) \n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
        "# setup tensorboard = ...\n",
        "# pake custom cb yang lihat ratio train acc dan val acc\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "# z = (x - u) / s\n",
        "\n",
        "# where u is the mean of the training samples or zero if with_mean=False, \n",
        "# and s is the standard deviation of the training samples or one if with_std=False.\n",
        "\n",
        "X_means = X_train.mean(axis=0)\n",
        "X_stds = X_train.std(axis=0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=100,\n",
        "          validation_data=(X_valid_scaled, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBDf1C1Q_Swu"
      },
      "source": [
        "Self normalizing architecture: 47.9\n",
        "\n",
        "Original Model: 47.6\n",
        "\n",
        "Batch Norm: 54"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkvtsrXi_gqA"
      },
      "source": [
        "## **e.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RthGpaCA_wIs"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100,\n",
        "                                 kernel_initializer=\"lecun_normal\",\n",
        "                                 activation=\"selu\"))\n",
        "\n",
        "# With an extensive hyperparameter search, it might be possible to do better \n",
        "# (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), \n",
        "# but probably not much better in this case. -> 48.9% accuracy\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(lr=5e-4) \n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
        "# setup tensorboard = ...\n",
        "# pake custom cb yang lihat ratio train acc dan val acc\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "X_means = X_train.mean(axis=0)\n",
        "X_stds = X_train.std(axis=0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=100,\n",
        "          validation_data=(X_valid_scaled, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
        "model.evaluate(X_valid_scaled, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKNDNQsABzZz"
      },
      "source": [
        "# https://towardsdatascience.com/monte-carlo-dropout-7fd52f8b6571#:~:text=Monte%20Carlo%20Dropout%20boils%20down,softmax%20outputs%20for%20each%20class.\n",
        "# https://towardsdatascience.com/12-main-dropout-methods-mathematical-and-visual-explanation-58cdc2112293\n",
        "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
        "    def call(self, inputs):\n",
        "        return super().call(inputs, training=True)   \n",
        "\n",
        "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
        "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
        "    return np.mean(Y_probas, axis=0)\n",
        "\n",
        "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
        "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
        "    return np.argmax(Y_probas, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZfxDwwxB7P0"
      },
      "source": [
        "mc_model = keras.models.Sequential([\n",
        "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
        "    for layer in model.layers\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYDiggZBB7KJ"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
        "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
        "accuracy # no improvement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqysH0qzCthH"
      },
      "source": [
        "## **f.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCsakrJ0Cvvv"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100,\n",
        "                                 kernel_initializer=\"lecun_normal\",\n",
        "                                 activation=\"selu\"))\n",
        "    \n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = keras.optimizers.SGD(lr=1e-3)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmwg6flkDCjW"
      },
      "source": [
        "batch_size = 128\n",
        "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
        "plot_lr_vs_loss(rates, losses)\n",
        "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s5lsTGlDO4o"
      },
      "source": [
        "class OneCycleScheduler(keras.callbacks.Callback):\n",
        "    def __init__(self, iterations, max_rate, start_rate=None,\n",
        "                 last_iterations=None, last_rate=None):\n",
        "        self.iterations = iterations\n",
        "        self.max_rate = max_rate\n",
        "        self.start_rate = start_rate or max_rate / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_rate = last_rate or self.start_rate / 1000\n",
        "        self.iteration = 0\n",
        "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
        "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
        "                / (iter2 - iter1) + rate1)\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
        "                                     self.max_rate, self.start_rate)\n",
        "        else:\n",
        "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
        "                                     self.start_rate, self.last_rate)\n",
        "        self.iteration += 1\n",
        "        K.set_value(self.model.optimizer.lr, rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuayLL74DsEc"
      },
      "source": [
        "n_epochs = 15\n",
        "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
        "                    validation_data=(X_valid_scaled, y_valid),\n",
        "                    callbacks=[onecycle])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}